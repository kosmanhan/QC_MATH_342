---
title: "Lab 7"
author: "Osman Khan"
output: pdf_document
date: 1st April 2024
---


#Polynomial Regression and Interaction Regression

We will work with the diamonds dataset again. Here we load up the dataset and convert all factors to nominal type:

```{r}
pacman::p_load(ggplot2) #this loads the diamonds data set too
?diamonds
diamonds
diamonds$cut =      factor(diamonds$cut, ordered = FALSE)      #convert to nominal
diamonds$color =    factor(diamonds$color, ordered = FALSE)    #convert to nominal
diamonds$clarity =  factor(diamonds$clarity, ordered = FALSE)  #convert to nominal
skimr::skim(diamonds)
```

Given the information above, what are the number of columns in the raw X matrix?

10

Verify this using code:

```{r}
#length(diamonds)
#nrow(diamonds)
dim(diamonds)[2]
```

Would it make sense to use polynomial expansions for the variables cut, color and clarity? Why or why not?

No. The three variables are would be dummified to be binaries. After that, no number of positive integer exponentiation would change the value of the binary. 


Would it make sense to use log transformations for the variables cut, color and clarity? Why or why not?

No. Log of zero is undefined, while log of 1 is zero. 

In order to ensure there is no time trend in the data, randomize the order of the diamond observations in D:.

```{r}
diamonds = diamonds[sample(1:nrow(diamonds)),]
```

Let's also concentrate only on diamonds with <= 2 carats to avoid the issue we saw with the maximum. So subset the dataset. Create a variable n equal to the number of remaining rows as this will be useful for later. Then plot it.

```{r}
diamonds=diamonds[diamonds$carat<=2,]
n=nrow(diamonds)
ggplot(diamonds, aes(x = carat, y = price)) + 
  geom_point(color = 'darkviolet',fill='darkgreen',alpha=1)
```

Create a linear model of price ~ carat and gauge its in-sample performance using s_e.

```{r}
summary(lm (price ~ carat, data = diamonds))$sigma
```

Create a model of price ~ clarity and gauge its in-sample performance

```{r}
summary(lm (price ~ clarity, data = diamonds))$sigma
```

Why is the model price ~ carat substantially more accurate than price ~ clarity?

Carat has more of an impact than clarity on price. Clarity is categorical while carat is continuos. 

Create a new transformed feature ln_carat and plot it vs price.

```{r}
diamonds$ln_carat=log(diamonds$carat)
ggplot(diamonds, aes(x = ln_carat, y = price)) + 
  geom_point(color='darkgoldenrod1')
```

Would price ~ ln_carat be a better fitting model than price ~ carat? Why or why not?

No. The graph does not appear to show a better linear relationship.

Verify this by comparing R^2 and RMSE of the two models:

```{r}
mod1=lm(price~carat, data = diamonds)
mod2=lm(price~ln_carat, data = diamonds)
summary(mod1)$r.squared
summary(mod1)$sigma
summary(mod2)$r.squared
summary(mod2)$sigma
```

Create a new transformed feature ln_price and plot its estimated density:


```{r}
diamonds$ln_price = log(diamonds$price)
ggplot(diamonds) + geom_histogram(aes(x = ln_price, color = 'purple'), binwidth = 0.01)
```


Now plot it vs carat.

```{r}
ggplot(diamonds, aes(x = carat, y = ln_price)) + 
  geom_point(color = 'navy')
```

Would ln_price ~ carat be a better fitting model than price ~ carat? Why or why not?

Yes! It looks more linear.

Verify this by computing s_e of this new model. Make sure these metrics can be compared apples-to-apples with the previous.

```{r}
mod3=lm(price~carat, data = diamonds)
mod4=lm(ln_price~carat, data = diamonds)
summary(mod3)$r.squared
summary(mod3)$sigma


y_hat=exp(mod4$fitted.values)
y= diamonds$price
e = y - y_hat
1-sum(e^2)/sum((y- mean(y))^2)#R-squared
sqrt(sum(e^2)/(mod2$df.residual))#RMSE of mod4
```
We just compared in-sample statistics to draw a conclusion on which model has better performance. But in-sample statistics can lie! Why is what we did valid?

We compared apples-apples, so what we did is valid. Plus, we have a large n v. a low p, which leads to low overfitting/misspecification error. 

Plot ln_price vs ln_carat.

```{r}
ggplot(diamonds, aes(x = ln_carat, y = ln_price)) + 
  geom_point(color='darkslategray2')
```

Would ln_price ~ ln_carat be the best fitting model than the previous three we considered? Why or why not?

Yes, as the graph appears to show that the model is linear. 

Verify this by computing s_e of this new model. Make sure these metrics can be compared apples-to-apples with the previous.

```{r}
#Model A, which used this annotation for help in the future
mod_a=lm(ln_price ~ ln_carat, data = diamonds)
y_hat=exp(mod_a$fitted.values)
y= diamonds$price
e = y - y_hat
1-sum(e^2)/sum((y- mean(y))^2)#R-squared
sqrt(sum(e^2)/(mod_a$df.residual))#RMSE
```

Compute b, the OLS slope coefficients for this new model of ln_price ~ ln_carat.

```{r}
coef(mod5)
```

Interpret b_1, the estimated slope of ln_carat.

If carat size increases by p%, then Price increases ~p%*1.7

Interpret b_0, the estimated intercept.

If ln_carat = 0, then carat =1. For a 1-carat diamond, the price is predicted to be ~e^(8.5).

Create other features ln_x, ln_y, ln_z, ln_depth, ln_table.

```{r}
diamonds$ln_x = log(diamonds$x)
diamonds$ln_y = log(diamonds$y)
diamonds$ln_z = log(diamonds$z)
diamonds$ln_depth = log(diamonds$depth)
diamonds$ln_table = log(diamonds$table)
```

From now on, we will be modeling ln_price (not raw price) as the prediction target. 

Create a model (B) of ln_price on ln_carat interacted with clarity and compare its performance with the model (A) ln_price ~ ln_carat.

```{r}
#Model B
mod_b = lm(ln_price ~ ln_carat*clarity, diamonds)
summary(mod_b)$sigma
summary(mod_a)$sigma
```

Which model does better? Why?

Model B does better, as we are adding the interaction of ln_carat with clarity on ln_price, rather than just ln_carat, and clarity is relevant. 

Create a model of (C) ln_price on ln_carat interacted with every categorical feature (clarity, cut and color) and compare its performance with model (B)

```{r}
#Model C
mod_c = lm(ln_price ~ ln_carat*clarity + cut + color, diamonds)
summary(mod_c)$sigma
```

Which model does better? Why?

Model C does better, as the three categorical features are relevant. 

Create a model (D) of ln_price on every continuous feature (logs of carat, x, y, z, depth, table) interacted with every categorical feature (clarity, cut and color) and compare its performance with model (C).

```{r}
#Model D
diamonds = diamonds[diamonds$x != 0 & diamonds$y != 0 & diamonds$z != 0 & diamonds$depth !=0 & diamonds$table != 0, ]
mod_d = lm(ln_price ~ (ln_carat + ln_x + ln_y + ln_z + ln_depth + ln_table)*(clarity + cut + color), diamonds)
summary(mod_d)$sigma
```

Which model does better? Why?

Model D does better. It is using all the features. At this point, we cannot know for sure whether this is overfitting or if these features are actually relevant. 

What is the p of this model D? Compute with code.

```{r}
mod_d$rank - 1
```

Create model (E) which is the same as before except create include the raw features interacted with the categorical features and gauge the performance against (D).

```{r}
#Model E
mod_e = lm(ln_price ~ (carat + x + y + z + depth + table)*(clarity + cut + color), diamonds)
summary(mod_e)$sigma
#mod_e$rank - 1, only difference is the non-logging, which is why p is the same
```

Which model does better? Why?

Both the models are the same, as at this point because there is so much overfitting, the performance is similar. 

Create model (F) which is the same as before except also include also third degree polynomials of the continuous features interacted with the categorical features and gauge performance against (E). By this time you're getting good with R's formula syntax!

```{r}
#Model F
mod_f = lm(ln_price ~ (poly(carat, 3) + poly(x,3) + poly(y,3) + poly(z,3) + poly(depth,3) + poly(table,3))*(clarity + cut + color), data = diamonds)
summary(mod_f)$sigma
#mod_f$rank - 1#, p is 341
```

Which model does better? Why?

Model F appears to do even better because this is even more overfitting. 

Can you think of any other way to expand the candidate set curlyH? Discuss.

We could add trignometric functions to the candidate set. 

We should probably assess oos performance now. Sample 2,000 diamonds and use these to create a training set of 1,800 random diamonds and a test set of 200 random diamonds. Define K and do this splitting:

```{r}
#I recreated the data set, as the previous set kept on giving me errors.
rm(list = ls())
pacman::p_load(ggplot2)
diamonds$cut =      factor(diamonds$cut, ordered = FALSE)
diamonds$color =    factor(diamonds$color, ordered = FALSE)
diamonds$clarity =  factor(diamonds$clarity, ordered = FALSE)
#drop some obviously nonsense observations
diamonds = diamonds[diamonds$carat <= 2 & diamonds$x != 0 & diamonds$y != 0 & diamonds$z != 0 & diamonds$depth != 0 & diamonds$table != 0,]
diamonds$ln_price = log(diamonds$price)
diamonds$ln_carat = log(diamonds$carat)
diamonds$ln_x = log(diamonds$x)
diamonds$ln_y = log(diamonds$y)
diamonds$ln_z = log(diamonds$z)
diamonds$ln_depth = log(diamonds$depth)
diamonds$ln_table = log(diamonds$table)
n = nrow(diamonds)
set.seed(1994)
diamonds = diamonds[sample(1 : n), ]

model_formulas = list(
  A = ln_price ~ ln_carat,
  B = ln_price ~ ln_carat * clarity,
  C = ln_price ~ ln_carat * (clarity + cut + color),
  D = ln_price ~ (ln_carat + ln_x + ln_y + ln_z + ln_depth + ln_table) * (clarity + cut + color),
  E = ln_price ~ (ln_carat + ln_x + ln_y + ln_z + ln_depth + ln_table + carat + x + y + z + depth + table) * (clarity + cut + color)
)

#Model A
mod = lm(model_formulas[["A"]], diamonds)
summary(mod)$sigma
mod$rank #i.e. degrees of freedom  = # vectors in colsp[X] to project onto
#Model B
mod = lm(model_formulas[["B"]], diamonds)
summary(mod)$sigma
mod$rank #i.e. degrees of freedom  = # vectors in colsp[X] to project onto
#Model C
mod = lm(model_formulas[["C"]], diamonds)
summary(mod)$sigma
mod$rank #i.e. degrees of freedom  = # vectors in colsp[X] to project onto
#Model D
mod = lm(model_formulas[["D"]], diamonds)
summary(mod)$sigma
mod$rank #i.e. degrees of freedom  = # vectors in colsp[X] to project onto
#Model E 
mod = lm(model_formulas[["E"]], diamonds)
summary(mod)$sigma
mod$rank #i.e. degrees of freedom  = # vectors in colsp[X] to project onto

#Model F
model_formulas[["F"]] = ln_price ~ 
        (ln_carat + ln_x + ln_y + ln_z + ln_depth + ln_table + poly(carat, 3) + poly(x, 3) + poly(y, 3) + poly(z, 3) + poly(depth, 3) + poly(table, 3)) * (cut + color + clarity)
mod = lm(model_formulas[["F"]], diamonds)
summary(mod)$sigma
mod$rank

K = 10
n_sub = 2000
n_test = 1 / K * n_sub
n_train = n_sub - n_test
test_indicies = sample(1 : n, n_test)
train_indicies = sample(setdiff(1 : n, test_indicies), n_train)
all_other_indicies = setdiff(1 : n, c(test_indicies, train_indicies))
```

#standard error of residuals

Compute in and out of sample performance for models A-F. Use s_e as the metric (standard error of the residuals). Create a list with keys A, B, ..., F to store these metrics. Remember the performances here will be worse than before since before you're using nearly 52,000 diamonds to build a model and now it's only 1,800! 

```{r}
oos_se = list()
all_models_train = list()
for (model_idx in LETTERS[1 : 6]){
  all_models_train[[model_idx]] = lm(model_formulas[[model_idx]], diamonds[train_indicies, ])
  summary(all_models_train[[model_idx]])$sigma
  oos_se[[model_idx]] = sd(diamonds$price[test_indicies] - predict(all_models_train[[model_idx]], diamonds[test_indicies, ]))
}
oos_se
```

You computed oos metrics only on n_* = 200 diamonds. What problem(s) do you expect in these oos metrics?

They are variable. And something is wrong with F! Possibly Runge's phenomenon?

To do the K-fold cross validation we need to get the splits right and crossing is hard. We've developed code for this already in a previous lab.

```{r}
temp = rnorm(n_sub)
folds_vec = cut(temp, breaks = quantile(temp, seq(0, 1, length.out = K + 1)), include.lowest = TRUE, labels = FALSE)
rm(temp)
head(folds_vec, 100)
```

Do the K-fold cross validation for model F and compute the overall s_e and s_s_e. 
```{r}
oos_se = list()
oos_s_se = list()
for (model_idx in LETTERS[1 : 6]){
  e_vec_k = list() #for each one
  for (k in 1 : K){
    test_indicies_k = which(folds_vec == k)
    train_indicies_k = which(folds_vec != k)
    mod = lm(model_formulas[[model_idx]], diamonds[train_indicies_k, ])
    e_vec_k[[k]] = sd(diamonds$price[test_indicies_k] - predict(mod, diamonds[test_indicies_k, ]))
  }
  oos_se[[model_idx]] = mean(unlist(e_vec_k)) #note: not exactly the overall sd, but close enough
  oos_s_se[[model_idx]] = sd(unlist(e_vec_k))
}
res = rbind(unlist(oos_se), unlist(oos_s_se))
rownames(res) = c("avg", "sd")
res
```

Does K-fold CV help reduce variance in the oos s_e? Discuss.

Yes, it helps reduce variance. This is especially visible for the sixth model. 

Imagine using the entire rest of the dataset besides the 2,000 training observations divvied up into slices of 200. Measure the oos error for each slice on Model F in a vector `s_e_s_F` and compute the `s_s_e_F` and also plot it.

```{r}
n_step = 1 / K * n_sub
oos_se = list()
ses = list()
starting_ks = seq(from = 1, to = (length(all_other_indicies) - n_step), by = n_step)
for (model_idx in LETTERS[1 : 6]){
  se_k = list() #for each one
  for (k in 1 : length(starting_ks)){
    diamonds_k = diamonds[all_other_indicies[starting_ks[k] : (starting_ks[k] + n_step - 1)], ]
    se_k[[k]] = sd(diamonds_k$price - predict(all_models_train[[model_idx]], diamonds_k))
  }
  ses[[model_idx]] = se_k
  oos_se[[model_idx]] = unlist(se_k)
}

pacman::p_load(reshape2)
ggplot(reshape2::melt(oos_se)) + geom_boxplot(aes(x = L1, y = value)) + xlab("model")
ggplot(reshape2::melt(oos_se)) + geom_boxplot(aes(x = L1, y = value)) + xlab("model") + ylim(0, 5000)
```


#Rcpp and optimizing R

Write a function `dot_product_R` in R that takes in two vectors `v1` and `v2` and returns their dot product.

```{r}
rm(list = ls())
pacman::p_load(Rcpp)
evalCpp("1 + 1", showOutput= TRUE)

dot_product_R=function(v1, v2){
  dot_product=0
  for(i in 1:length(v1)){
    dot_product = dot_product + v1[i]*v2[i]
  }
  dot_product
}
dot_product_R(c(1,5),c(2,6))
```

Write a function `dot_product_cpp` in C++ and make sure it compiles.

```{r}
cppFunction('
  double dot_product_cpp(NumericVector v1, NumericVector v2) {
  int dot_product=0;
  for(int i = 0; i < v1.length(); i++){
    dot_product = dot_product + (v1[i] * v2[i]);
  }
  return dot_product;
}
')
v3=c(1,5)
v4=c(2,6)
dot_product_cpp(v3,v4)
evalCpp(dot_product_cpp(v3,v4))
?evalCpp
```

Create two vectors of standard normal realizations with length `n=1e6` and test the different in speed.

```{r}
n = 1e6
v1 = rnorm(n)
v2 = rnorm(n)

pacman::p_load(microbenchmark)
microbenchmark(
  dot_product_R(v1, v2), 
  dot_product_cpp(v1, v2),
  times = 10
)
```

Implement the Gram Schmidt routine as a C++ function `gram_schmidt_cpp`.

```{r}
#Rcpp::transpose()
#Rcpp::pow(v_k, 2)
#Rcpp::sum(Rcpp::pow(V.column(j), 2))
cppFunction('
  NumericMatrix gram_schmidt_cpp(NumericMatrix X){
  
  NumericMatrix V(X.nrow(), X.ncol());
  
  for (int i = 0; i < X.nrow(); i++){
    V(i,0) = X(i,0);
  }
  
  
  for (int j = 1; j < X.ncol(); j++){
    for (int i = 0; i< X.nrow(); i++){
      V(i,j) = X(i,j);
    }
    
    for (int k = 0; k<j; k++){
      NumericMatrix v_k = V[,k];
      V(_, j) = V(_, j) - (t(t(v_k)) %*% t(v_k) / sum(v_k^2)) %*% t(t(X(_, j));
    }
  }

  NumericMatrix Q(X.nrow(),X.ncol());
  
  for (int j = 1; j < X.ncol(); j++){
    Q(_, j) = V(_, j) / sqrt(sum(V[_, j)^2));
  }
  return NumericMatrix Q;
}
  
')



```

Here is the implementation in R for reference taken from lab 5:

```{r}
gram_schmidt_R = function(X){
  #first create orthogonal matrix
  V = matrix(NA, 
             nrow = nrow(X), 
             ncol = ncol(X))

    V[, 1] = X[, 1]
  
  for (j in 2 : ncol(X)){
    V[, j] = X[, j]
    
    for (k in 1 : (j-1)){
      v_k = V[, k, drop = FALSE]
      
      V[, j] = V[, j, drop = FALSE] - (t(t(v_k)) %*% t(v_k) / sum(v_k^2)) %*% t(t(X[, j])) #i.e. the orthogonal projection of X[, j] onto v_k
    }
  }
  
  Q = matrix(NA, nrow = nrow(X), 
              ncol = ncol(X))
  for (j in 1 : ncol(X)){
    Q[, j] = V[, j] / sqrt(sum(V[, j]^2))
  }
  Q
}
```

Now let's see how much faster C++ is by running it on the boston housing data design matrix
```{r}
X = model.matrix(medv ~ ., MASS::Boston)

microbenchmark(
  gram_schmidt_R(X),
  gram_schmidt_cpp(X),
  times = 10
)
```


