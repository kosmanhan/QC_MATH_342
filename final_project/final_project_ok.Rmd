---
title: "Final_Project_MATH_642"
author: "Osman Khan"
date: "2024-05-26"
---

```{r}
#clean the workspace
rm(list = ls())
# Load necessary libraries
library(magrittr)   # To pipe the data.frames
library(tidyverse)  # A collection of R packages for data manipulation and visualization
library(readr)      # For reading CSV files
pacman::p_load(missForest) #For imputation
```

```{r}
#for using Regression Tree & Random Forest
if (!pacman::p_isinstalled(YARF)){
  pacman::p_install_gh("kapelner/YARF/YARFJARs", ref = "dev")
  pacman::p_install_gh("kapelner/YARF/YARF", ref = "dev", force = TRUE)
}
options(java.parameters = "-Xmx4000m")
pacman::p_load(YARF)
```

```{r}
pacman::p_load(xtable)
```


```{r}
#Load the dataset
housing_data = read_csv('housing_data_2016_2017.csv', show_col_types = FALSE)
```


```{r}
#columns not necessary
unnecessary_columns = c('HITId', 'HITTypeId', 'Title', 'Description', 'Keywords', 'Reward', 'CreationTime', 'MaxAssignments', 'RequesterAnnotation', 'AssignmentDurationInSeconds', 'AutoApprovalDelayInSeconds', 'Expiration', 'NumberOfSimilarHITs', 'LifetimeInSeconds', 'AssignmentId', 'WorkerId', 'AssignmentStatus', 'AcceptTime', 'SubmitTime', 'AutoApprovalTime', 'ApprovalTime', 'RejectionTime', 'RequesterFeedback', 'WorkTimeInSeconds', 'LifetimeApprovalRate', 'Last30DaysApprovalRate', 'Last7DaysApprovalRate','listing_price_to_nearest_1000', 'URL', 'url')
unique(housing_data$garage_exists)
#removing unnecessary columns
housing_25=housing_data %>% 
  select(-all_of(unnecessary_columns))
skimr::skim(housing_data)
```
Our target is sale_price, so we only take the subset of columns that have a value for sale_price.
```{r}
#remove rows without a sale_price
housing_25_528 = housing_25 %>% drop_na(sale_price)
#skimr::skim(housing_25_528)
```
Changing the data types of the 25 columns to the appropriate ones
```{r}
#cleaning the following for spelling and other errors
housing_25_528$fuel_type=replace(housing_25_528$fuel_type,housing_25_528$fuel_type %in% c('none','Other'),'other')
housing_25_528$garage_exists=replace(housing_25_528$garage_exists,housing_25_528$garage_exists %in% c('Underground','Yes','UG','1','eys'),'yes')
housing_25_528$kitchen_type=replace(housing_25_528$kitchen_type,housing_25_528$kitchen_type %in% c("eat in","Eat In","Eat in"),'eat-in')
housing_25_528$kitchen_type=replace(housing_25_528$kitchen_type,housing_25_528$kitchen_type == 'Combo', 'combo')
housing_25_528$model_type=as.factor(housing_25_528$model_type)

#no change needed: pct_tax_deductibl, sq_footage

#following are factors
housing_25_528$community_district_num=as.factor(housing_25_528$community_district_num)
housing_25_528$date_of_sale=as.factor(housing_25_528$date_of_sale)
housing_25_528$dining_room_type=as.factor(housing_25_528$dining_room_type)
housing_25_528$fuel_type=as.factor(housing_25_528$fuel_type)
housing_25_528$full_address_or_zip_code = as.factor(housing_25_528$full_address_or_zip_code)
housing_25_528$garage_exists=as.factor(housing_25_528$garage_exists)
housing_25_528$kitchen_type=as.factor(housing_25_528$kitchen_type)

#binary factors
housing_25_528$cats_allowed = ifelse(housing_25_528$cats_allowed == 'no', 0, 1)
housing_25_528$cats_allowed=as.factor(housing_25_528$cats_allowed)
housing_25_528$dogs_allowed = ifelse(housing_25_528$dogs_allowed == 'no', 0, 1)
housing_25_528$dogs_allowed=as.factor(housing_25_528$dogs_allowed)
housing_25_528$coop_condo=as.factor(housing_25_528$coop_condo)
housing_25_528$garage_exists= ifelse(is.na(housing_25_528$garage_exists), 'no','yes')
housing_25_528$garage_exists=as.factor(housing_25_528$garage_exists)

#converting to numeric (by removing $ & commas)
housing_25_528$common_charges=parse_number(housing_25_528$common_charges)
housing_25_528$maintenance_cost=parse_number(housing_25_528$maintenance_cost)
housing_25_528$parking_charges=parse_number(housing_25_528$parking_charges)
housing_25_528$sale_price=parse_number(housing_25_528$sale_price)
housing_25_528$total_taxes=parse_number(housing_25_528$total_taxes)

#Display the first few rows of the dataset
#head(housing_data)
#summary(housing_data)
#skimr::skim(housing_25_528)
```


```{r}
#we need logical categories. i) scrap model type as there are 356 unique values, ii) extract zipcode from address, and then bin it to one of the 9 areas of queens, iii) extract month of sale from date of sale, iv) bin number of floors, v) bin approx_year_built into mostly decades (10-s), vi) bin walk scores into 20-s.
#i
housing_24=housing_25_528 %>% select(-model_type)
#ii
housing_24$zip_code=str_extract(housing_24$full_address_or_zip_code,"[0-9]{5}") 
housing_24 %<>% select(-full_address_or_zip_code)
housing_24$zip_code=as.numeric(housing_24$zip_code)

#iii
housing_24$month_of_sale=month(as.Date(housing_24$date_of_sale,format='%m/%d/%Y'))
housing_24 %<>% select(-date_of_sale)
```                

```{r}
#create a matrix, M, that includes missingness
M = as_tibble(apply(is.na(housing_24), 2, as.numeric))
colnames(M) = paste("is_missing_", colnames(housing_24), sep = "")
M %<>% 
  select_if(function(x){sum(x) > 0})
head(M)
skimr::skim(M)
```


```{r}
#we impute, by creating a matrix, Ximp, that uses missForest
Ximp=missForest(data.frame(housing_24))$ximp
skimr::skim(Ximp)
```
```{r}
#we combine Ximp with M to get Xy
Xy=cbind(Ximp,M)
```


```{r}
#now we can bin
#ii
Xy %<>% mutate(bin_zip = cut(zip_code, breaks=c(11003,11005,11106,11360,11364,11367,11378,11385,11421,11429,11436)))
Xy %<>% select(-zip_code)

#iv
Xy %<>% mutate(bin_floor = cut(num_floors_in_building, breaks=c(0,4,7,10,16,25,35)))
Xy %<>% select(-num_floors_in_building)

#v
Xy %<>% mutate(bin_decade_built = cut(approx_year_built, breaks=c(1893,1911,1920,1930,1940,1950,1960,1970,1980,1990,2000,2010,2020)))
Xy %<>% select(-approx_year_built)
Xy$is_missing_approx_decade_built=Xy$is_missing_approx_year_built
Xy %<>% select(-is_missing_approx_year_built)

#vi
Xy %<>% mutate(bin_walk_score = cut(walk_score, breaks=c(0,20,40,60,80,100)))
Xy %<>% select(-walk_score)


#remove those zipcodes that are out of our range
Xy %<>% drop_na(bin_zip)

```


```{r}
#we convert the five numbers to factor: num_bedrooms, num_full_bathroom, num_half_bathrooms, num_total_rooms, month_of_sale
Xy$num_total_rooms=as.factor(Xy$num_total_rooms)
Xy$num_bedrooms=as.factor(Xy$num_bedrooms)
Xy$num_full_bathrooms=as.factor(Xy$num_full_bathroom)
Xy$num_half_bathrooms=as.integer(Xy$num_half_bathrooms)
Xy$num_half_bathrooms=as.factor(Xy$num_half_bathrooms)
Xy$month_of_sale=as.factor(Xy$month_of_sale)
#convert 13 missing indicators to factors
Xy$is_missing_approx_decade_built =as.factor(Xy$is_missing_approx_decade_built)
Xy$is_missing_common_charges =as.factor(Xy$is_missing_common_charges)
Xy$is_missing_community_district_num =as.factor(Xy$is_missing_community_district_num)
Xy$is_missing_dining_room_type =as.factor(Xy$is_missing_dining_room_type)
Xy$is_missing_fuel_type =as.factor(Xy$is_missing_fuel_type)
Xy$is_missing_kitchen_type =as.factor(Xy$is_missing_kitchen_type)
Xy$is_missing_maintenance_cost =as.factor(Xy$is_missing_maintenance_cost)
Xy$is_missing_num_floors_in_building =as.factor(Xy$is_missing_num_floors_in_building)
Xy$is_missing_num_half_bathrooms =as.factor(Xy$is_missing_num_half_bathrooms)
Xy$is_missing_parking_charges =as.factor(Xy$is_missing_parking_charges)
Xy$is_missing_pct_tax_deductibl =as.factor(Xy$is_missing_pct_tax_deductibl)
Xy$is_missing_sq_footage =as.factor(Xy$is_missing_sq_footage)
Xy$is_missing_total_taxes =as.factor(Xy$is_missing_total_taxes)
#skimr::skim(Xy)
```

```{r}
#Regression Tree
#we build training, & test sets.
#training_test 400 v. 121 split
test_indices=sample(1 : nrow(Xy), 121)
train_indices = setdiff(1 : nrow(Xy), test_indices)
Xy_train = Xy[train_indices, ]
y_train = Xy_train$sale_price
X_train = Xy_train
X_train$sale_price = NULL
n_train = nrow(X_train)
Xy_test = Xy[test_indices, ]
y_test = Xy_test$sale_price
X_test = Xy_test
X_test$sale_price = NULL

tree_mod = YARFCART(X_train, y_train, calculate_oob_error = FALSE)
#in-sample stats
y_hat_rt_train = predict(tree_mod, X_train)
e_rt = y_train - y_hat_rt_train
sd(e_rt)#in-sample rmse
1 - sd(e_rt) / sd(y_train)#in-sample r-squared
illustrate_trees(tree_mod, max_depth = 6, margin_in_px= 100, length_in_px_per_half_split = 40,open_file = TRUE)
?illustrate_trees
get_tree_num_nodes_leaves_max_depths(tree_mod)
#oos stats
y_hat_rt_test = predict(tree_mod, X_test)
e_rt_oos = y_test - y_hat_rt_test
sd(e_rt_oos)#oos rmse
1 - sd(e_rt_oos) / sd(y_test)#oos r-squared
```


```{r}
#Vanilla OLS, which we will do with 5 different test sets
#convert to numeric, otherwise OLS does not work for oos
vanillaXy=lapply(Xy,as.numeric)
vanillaXy=as.data.frame(vanillaXy)
y=vanillaXy$sale_price
X=vanillaXy %>% select(-sale_price)
t=10
vanilla_in_rmse=c(rep(NA,t))
vanilla_in_r2=c(rep(NA,t))
vanilla_oos_rmse=c(rep(NA,t))
vanilla_oos_r2=c(rep(NA,t))

#training_test 400 v. 121 split
for(i in 1:t){
  test_indices=sample(1 : nrow(vanillaXy), 121)
  train_indices = setdiff(1 : nrow(vanillaXy), test_indices)
  Xy_train = vanillaXy[train_indices, ]
  y_train = Xy_train$sale_price
  X_train = Xy_train
  X_train$sale_price = NULL
  n_train = nrow(X_train)
  Xy_test = vanillaXy[test_indices, ]
  y_test = Xy_test$sale_price
  X_test = Xy_test
  X_test$sale_price = NULL
  vanilla_mod = lm(y_train ~ ., X_train)
  #in-sample stats
  y_hat_train = predict(vanilla_mod, X_train)
  e = y_train - y_hat_train
  vanilla_in_rmse[i]=sd(e)#in-sample rmse
  vanilla_in_r2[i]=1 - sd(e) / sd(y_train)#in-sample r-squared
  #oos stats
  y_hat_test = predict(vanilla_mod, X_test)
  e_oos = y_test - y_hat_test
  vanilla_oos_rmse[i]=sd(e_oos)#oos rmse
  vanilla_oos_r2[i]=1 - sd(e_oos) / sd(y_test)#oos r-squared
}

summary(vanilla_mod)#for the table

mean(vanilla_in_rmse)
mean(vanilla_in_r2)
mean(vanilla_oos_rmse)
mean(vanilla_oos_r2)
library(xtable)
newobject<-xtable(summary(vanilla_mod))
print.xtable(newobject, type='latex', file='filename.tex')
```

```{r}
#Random Forest
#skimr::skim(Xy)
rf_in_rmse=c(rep(NA,5))
rf_in_r2=c(rep(NA,5))
rf_oos_rmse=c(rep(NA,5))
rf_oos_r2=c(rep(NA,5))

#training_test 400 v. 121 split
for(i in 1:5){
  test_indices=sample(1 : nrow(Xy), 121)
  train_indices = setdiff(1 : nrow(Xy), test_indices)
  Xy_train = Xy[train_indices, ]
  y_train = Xy_train$sale_price
  X_train = Xy_train
  X_train$sale_price = NULL
  Xy_test = Xy[test_indices, ]
  y_test = Xy_test$sale_price
  X_test = Xy_test
  X_test$sale_price = NULL
  yarf_mod = YARF(X_train, y_train, mtry=65)
  #in-sample stats
  y_hat_rf_train = predict(yarf_mod, X_train)
  e_rf = y_train - y_hat_rf_train
  rf_in_rmse[i]=sd(e_rf)#in-sample rmse
  rf_in_r2[i]=1 - sd(e_rf) / sd(y_train)#in-sample r-squared
  #oos stats
  y_hat_rf_test = predict(yarf_mod, X_test)
  e_rf_oos = y_test - y_hat_rf_test
  rf_oos_rmse[i]=sd(e_rf_oos)#oos rmse
  rf_oos_r2[i]=1 - sd(e_rf_oos) / sd(y_test)#oos r-squared
}
mean(rf_in_rmse)
mean(rf_in_r2)
mean(rf_oos_rmse)
mean(rf_oos_r2)
```


